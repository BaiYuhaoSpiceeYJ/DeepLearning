{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#VGG16.npy的保存形式是numpy的矩阵，和TensorFlow的保存形式不一样，需要重新解析\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939,116.779,123.68]#VGG上RGB的三个均值\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"创建VGG16网络结构，再从模型文件中读取参数\"\"\"\n",
    "    def  __init__(self,data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_filter(self, name):#获取文件中卷积层w的参数\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):#获取文件中全连接层w的参数\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):#获取文件中b的参数\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])\n",
    "            return x\n",
    "    \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\n",
    "        Parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('building model ...')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)#输入，切分的通道，切分的维度\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "            axis = 3)\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "        \n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation=None)\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        print('building model finished: %4ds' % (time.time() - start_time))#查看构建时间\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义输入输出超参数\n",
    "vgg16_npy_path = '.\\\\vgg16.npy'\n",
    "content_img_path = '.\\\\gugong2.jpg'\n",
    "style_img_path = '.\\\\gugong2.jpg'\n",
    "broken_img_path = '.\\\\gugong3.jpg'\n",
    "\n",
    "num_steps = 100\n",
    "learning_rate = 10\n",
    "\n",
    "lambda_c = 0.1\n",
    "lambda_s = 500\n",
    "\n",
    "output_dir = '.\\\\run_style_transfer'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model ...\n",
      "building model finished:    0s\n",
      "building model ...\n",
      "building model finished:    0s\n",
      "building model ...\n",
      "building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "def initial_result():#图像大小，均值，方差,构建初始化图像\n",
    "    #initial = tf.truncated_normal((1, 224, 224, 3), 127.5, 20)\n",
    "    initial = read_img(broken_img_path)\n",
    "    initial = tf.cast(initial, tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def read_img(img_name):#读取图片\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # (1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):#计算gram矩阵，两张特征图对应行向量两两求余弦相似度\n",
    "    \"\"\"Calulates gram matrix\n",
    "    Args:\n",
    "    - x: feaures extracted from VGG Net. shape: [1, width, height, ch]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch] 先做一次转置\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)#adjoint_a转置，做除法避免值过大\n",
    "    return gram\n",
    "    \n",
    "\n",
    "\n",
    "result = initial_result()\n",
    "\n",
    "\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "#给风格图像的每层卷积计算gram矩阵\n",
    "\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "#给结果风格图像的每层卷积计算gram矩阵\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1, 2], [3, 4], zip([1,2], [3,4]) -> [(1, 3), (2, 4)]\n",
    "# shape: [1, width, height, channel]\n",
    "for c, c_ in zip(content_features, result_content_features):#内容特征损失函数\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])#在1,2,3通道上求平均\n",
    "    \n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):#风格特征损失函数\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 6945.4160, content_loss: 59663.6250, style_loss:   1.9581\n",
      "step: 2, loss_value: 12417.1953, content_loss: 89505.4844, style_loss:   6.9333\n",
      "step: 3, loss_value: 11411.8887, content_loss: 89666.1719, style_loss:   4.8905\n",
      "step: 4, loss_value: 9666.9814, content_loss: 84407.7031, style_loss:   2.4524\n",
      "step: 5, loss_value: 8795.7080, content_loss: 79501.4531, style_loss:   1.6911\n",
      "step: 6, loss_value: 8215.5234, content_loss: 74226.5469, style_loss:   1.5857\n",
      "step: 7, loss_value: 7316.9668, content_loss: 67490.0547, style_loss:   1.1359\n",
      "step: 8, loss_value: 6446.8955, content_loss: 60005.8086, style_loss:   0.8926\n",
      "step: 9, loss_value: 5634.7681, content_loss: 52740.9375, style_loss:   0.7213\n",
      "step: 10, loss_value: 4912.0586, content_loss: 46312.3359, style_loss:   0.5616\n",
      "step: 11, loss_value: 4335.8809, content_loss: 41034.7070, style_loss:   0.4648\n",
      "step: 12, loss_value: 3882.0132, content_loss: 36669.6367, style_loss:   0.4301\n",
      "step: 13, loss_value: 3483.1064, content_loss: 32923.9844, style_loss:   0.3814\n",
      "step: 14, loss_value: 3141.2075, content_loss: 29689.1680, style_loss:   0.3446\n",
      "step: 15, loss_value: 2839.3943, content_loss: 26804.0000, style_loss:   0.3180\n",
      "step: 16, loss_value: 2564.5854, content_loss: 24248.0078, style_loss:   0.2796\n",
      "step: 17, loss_value: 2331.0176, content_loss: 22074.5469, style_loss:   0.2471\n",
      "step: 18, loss_value: 2127.7874, content_loss: 20157.9375, style_loss:   0.2240\n",
      "step: 19, loss_value: 1941.9867, content_loss: 18408.8164, style_loss:   0.2022\n",
      "step: 20, loss_value: 1768.7034, content_loss: 16796.3242, style_loss:   0.1781\n",
      "step: 21, loss_value: 1611.3252, content_loss: 15328.8066, style_loss:   0.1569\n",
      "step: 22, loss_value: 1472.5493, content_loss: 14013.4639, style_loss:   0.1424\n",
      "step: 23, loss_value: 1347.4703, content_loss: 12841.8818, style_loss:   0.1266\n",
      "step: 24, loss_value: 1234.6990, content_loss: 11783.3652, style_loss:   0.1127\n",
      "step: 25, loss_value: 1130.3962, content_loss: 10794.1289, style_loss:   0.1020\n",
      "step: 26, loss_value: 1035.5459, content_loss: 9890.6689, style_loss:   0.0930\n",
      "step: 27, loss_value: 947.8891, content_loss: 9053.2148, style_loss:   0.0851\n",
      "step: 28, loss_value: 870.2852, content_loss: 8293.4590, style_loss:   0.0819\n",
      "step: 29, loss_value: 797.0549, content_loss: 7591.5581, style_loss:   0.0758\n",
      "step: 30, loss_value: 747.9755, content_loss: 6984.9443, style_loss:   0.0990\n",
      "step: 31, loss_value: 686.7961, content_loss: 6470.3120, style_loss:   0.0795\n",
      "step: 32, loss_value: 653.8254, content_loss: 6095.6450, style_loss:   0.0885\n",
      "step: 33, loss_value: 603.6443, content_loss: 5679.4268, style_loss:   0.0714\n",
      "step: 34, loss_value: 554.9358, content_loss: 5199.5859, style_loss:   0.0700\n",
      "step: 35, loss_value: 514.8743, content_loss: 4724.2036, style_loss:   0.0849\n",
      "step: 36, loss_value: 458.1337, content_loss: 4302.4453, style_loss:   0.0558\n",
      "step: 37, loss_value: 423.0951, content_loss: 3985.2224, style_loss:   0.0491\n",
      "step: 38, loss_value: 392.3115, content_loss: 3675.0801, style_loss:   0.0496\n",
      "step: 39, loss_value: 354.9846, content_loss: 3340.1477, style_loss:   0.0419\n",
      "step: 40, loss_value: 335.9109, content_loss: 3062.7158, style_loss:   0.0593\n",
      "step: 41, loss_value: 310.7303, content_loss: 2858.7800, style_loss:   0.0497\n",
      "step: 42, loss_value: 312.6162, content_loss: 2781.0066, style_loss:   0.0690\n",
      "step: 43, loss_value: 297.2382, content_loss: 2636.5090, style_loss:   0.0672\n",
      "step: 44, loss_value: 282.9209, content_loss: 2469.3962, style_loss:   0.0720\n",
      "step: 45, loss_value: 264.4448, content_loss: 2261.7041, style_loss:   0.0765\n",
      "step: 46, loss_value: 259.2860, content_loss: 2094.4753, style_loss:   0.0997\n",
      "step: 47, loss_value: 223.7120, content_loss: 1903.0508, style_loss:   0.0668\n",
      "step: 48, loss_value: 212.8969, content_loss: 1831.9867, style_loss:   0.0594\n",
      "step: 49, loss_value: 202.8489, content_loss: 1767.6527, style_loss:   0.0522\n",
      "step: 50, loss_value: 216.4636, content_loss: 1759.8497, style_loss:   0.0810\n",
      "step: 51, loss_value: 196.2642, content_loss: 1715.9189, style_loss:   0.0493\n",
      "step: 52, loss_value: 188.9480, content_loss: 1694.8671, style_loss:   0.0389\n",
      "step: 53, loss_value: 181.4849, content_loss: 1594.4989, style_loss:   0.0441\n",
      "step: 54, loss_value: 155.6950, content_loss: 1398.6218, style_loss:   0.0317\n",
      "step: 55, loss_value: 151.3168, content_loss: 1259.2610, style_loss:   0.0508\n",
      "step: 56, loss_value: 156.3477, content_loss: 1202.4174, style_loss:   0.0722\n",
      "step: 57, loss_value: 194.8108, content_loss: 1273.9607, style_loss:   0.1348\n",
      "step: 58, loss_value: 198.1420, content_loss: 1314.3110, style_loss:   0.1334\n",
      "step: 59, loss_value: 223.1503, content_loss: 1375.4324, style_loss:   0.1712\n",
      "step: 60, loss_value: 164.2987, content_loss: 1285.5796, style_loss:   0.0715\n",
      "step: 61, loss_value: 165.6849, content_loss: 1232.7682, style_loss:   0.0848\n",
      "step: 62, loss_value: 155.1547, content_loss: 1198.5059, style_loss:   0.0706\n",
      "step: 63, loss_value: 187.2135, content_loss: 1326.2656, style_loss:   0.1092\n",
      "step: 64, loss_value: 180.9780, content_loss: 1442.4302, style_loss:   0.0735\n",
      "step: 65, loss_value: 185.1665, content_loss: 1498.7811, style_loss:   0.0706\n",
      "step: 66, loss_value: 191.6444, content_loss: 1439.6102, style_loss:   0.0954\n",
      "step: 67, loss_value: 152.5700, content_loss: 1262.2600, style_loss:   0.0527\n",
      "step: 68, loss_value: 150.2628, content_loss: 1187.7303, style_loss:   0.0630\n",
      "step: 69, loss_value: 147.7277, content_loss: 1097.8232, style_loss:   0.0759\n",
      "step: 70, loss_value: 142.0038, content_loss: 1047.4844, style_loss:   0.0745\n",
      "step: 71, loss_value: 145.4663, content_loss: 1061.0465, style_loss:   0.0787\n",
      "step: 72, loss_value: 140.7257, content_loss: 1069.7148, style_loss:   0.0675\n",
      "step: 73, loss_value: 121.3567, content_loss: 994.1833, style_loss:   0.0439\n",
      "step: 74, loss_value: 112.1617, content_loss: 916.4225, style_loss:   0.0410\n",
      "step: 75, loss_value: 100.6951, content_loss: 801.8725, style_loss:   0.0410\n",
      "step: 76, loss_value: 133.9396, content_loss: 843.9658, style_loss:   0.0991\n",
      "step: 77, loss_value: 119.0354, content_loss: 856.1282, style_loss:   0.0668\n",
      "step: 78, loss_value: 143.3147, content_loss: 986.0122, style_loss:   0.0894\n",
      "step: 79, loss_value: 169.3543, content_loss: 1038.2822, style_loss:   0.1311\n",
      "step: 80, loss_value: 181.5449, content_loss: 1057.5575, style_loss:   0.1516\n",
      "step: 81, loss_value: 151.5246, content_loss: 1035.4857, style_loss:   0.0960\n",
      "step: 82, loss_value: 152.9864, content_loss: 1040.6117, style_loss:   0.0979\n",
      "step: 83, loss_value: 127.0964, content_loss: 1007.5585, style_loss:   0.0527\n",
      "step: 84, loss_value: 132.9819, content_loss: 1066.0967, style_loss:   0.0527\n",
      "step: 85, loss_value: 129.9958, content_loss: 1029.5663, style_loss:   0.0541\n",
      "step: 86, loss_value: 117.7506, content_loss: 930.0957, style_loss:   0.0495\n",
      "step: 87, loss_value: 147.3746, content_loss: 917.7200, style_loss:   0.1112\n",
      "step: 88, loss_value: 145.9236, content_loss: 905.9549, style_loss:   0.1107\n",
      "step: 89, loss_value: 226.8539, content_loss: 1146.3722, style_loss:   0.2244\n",
      "step: 90, loss_value: 222.6699, content_loss: 1199.1639, style_loss:   0.2055\n",
      "step: 91, loss_value: 229.2783, content_loss: 1290.0509, style_loss:   0.2005\n",
      "step: 92, loss_value: 150.2567, content_loss: 1196.0021, style_loss:   0.0613\n",
      "step: 93, loss_value: 170.3417, content_loss: 1204.8518, style_loss:   0.0997\n",
      "step: 94, loss_value: 176.0823, content_loss: 1223.2222, style_loss:   0.1075\n",
      "step: 95, loss_value: 175.0898, content_loss: 1237.3867, style_loss:   0.1027\n",
      "step: 96, loss_value: 193.6541, content_loss: 1285.6395, style_loss:   0.1302\n",
      "step: 97, loss_value: 169.8606, content_loss: 1171.2599, style_loss:   0.1055\n",
      "step: 98, loss_value: 180.4178, content_loss: 1187.3838, style_loss:   0.1234\n",
      "step: 99, loss_value: 157.6781, content_loss: 1211.5798, style_loss:   0.0730\n",
      "step: 100, loss_value: 165.9359, content_loss: 1302.1299, style_loss:   0.0714\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content: content_val,\n",
    "                         style: style_val,\n",
    "                     })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \n",
    "              % (step+1,\n",
    "                 loss_value[0],#0是因为创建的时候是一个1个元素的数组，要用索引\n",
    "                 content_loss_value[0],\n",
    "                 style_loss_value[0]))\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step+1))\n",
    "        result_val = result.eval(sess)[0]#这个函数可以直接取出result变量，1,224，224，3，取出0个元素变成224,224,3\n",
    "        result_val = np.clip(result_val, 0, 255)#小于0变为0，大于255变成255\n",
    "        img_arr = np.asarray(result_val, np.uint8)#一定要设成unit8\n",
    "        img = Image.fromarray(img_arr)#将它写成图片，将数组转成image对象\n",
    "        img.save(result_img_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
