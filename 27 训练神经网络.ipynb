{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. 生成数据：载入词表，载入图像特征，提供数据\n",
    "2. 实现模型\n",
    "3. 训练\n",
    "4. 评估\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "input_description_file = \".\\\\data\\\\results_20130124.token\"\n",
    "input_img_feature_dir = \".\\\\InceptionV3\\\\feature_extraction_inception_v3\"\n",
    "input_vocab_file = \".\\\\data\\\\vocab.txt\"\n",
    "output_dir = \".\\\\data\\\\local_run\"\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(#将图像特征embedding\n",
    "        num_vocab_word_threshold=3,#过滤词表\n",
    "        num_embedding_nodes=32,#embedding的size,每张图变成32位embedding\n",
    "        num_timesteps=20,\n",
    "        num_lstm_nodes=[64, 64],#每层大小\n",
    "        num_lstm_layers=2,#LSTM层数\n",
    "        num_fc_nodes=32,#全连接的大小\n",
    "        batch_size=50,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,#梯度剪切\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=0.8,#Dropout\n",
    "        log_frequent=100,#每隔100打印一次log\n",
    "        save_frequent=1000,#每1000次保存一次模型\n",
    "    )\n",
    "\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "[1494, 389, 1, 0]\n",
      "'the of man white'\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "[[3, 9, 4, 132, 8, 3532, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "#词表载入以及文本描述文件转换成id\n",
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1#end of sentence\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename) \n",
    "\n",
    "    def _read_dict(self, filename):#3.读取词表，从词表文件中读取到新建的词典里\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)#对没有见过的词返回unk的id\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')#对于没有见过的id返回unk\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]#把句子转成id列表\n",
    "        return word_ids\n",
    "\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]#把id列表转成文本的一句话\n",
    "        return ' '.join(words)#返回字符串，用空格拼接\n",
    "    \n",
    "\n",
    "def parse_token_file(token_file):#将描述文件解析成字典\n",
    "    \"\"\"Parses token file.\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):#将图片的描述文件转成id\n",
    "    \"\"\"Converts tokens of each description of imgs to id. \"\"\"\n",
    "    img_name_to_token_ids = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_token_ids.setdefault(img_name, [])\n",
    "        descriptions = img_name_to_tokens[img_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_token_ids[img_name].append(token_ids)\n",
    "    return img_name_to_token_ids\n",
    "\n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "pprint.pprint(vocab.encode(\"I have a dream.\"))\n",
    "pprint.pprint(vocab.decode([5,10,9,20]))\n",
    "    \n",
    "    \n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_token_ids = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:10])\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_ids))\n",
    "pprint.pprint(list(img_name_to_token_ids.keys())[0:10])\n",
    "pprint.pprint(img_name_to_token_ids['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-0.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-1.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-10.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-11.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-12.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-13.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-14.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-15.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-16.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-17.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-18.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-19.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-2.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-20.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-21.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-22.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-23.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-24.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-25.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-26.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-27.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-28.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-29.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-3.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-30.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-31.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-4.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-5.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-6.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-7.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-8.pickle',\n",
      " '.\\\\InceptionV3\\\\feature_extraction_inception_v3\\\\image_features-9.pickle']\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-0.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-1.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-10.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-11.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-12.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-13.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-14.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-15.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-16.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-17.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-18.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-19.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-2.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-20.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-21.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-22.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-23.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-24.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-25.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-26.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-27.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-28.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-29.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-3.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-30.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-31.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-4.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-5.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-6.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-7.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-8.pickle\n",
      "INFO:tensorflow:loading .\\InceptionV3\\feature_extraction_inception_v3\\image_features-9.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.41847968, 0.12264318, 0.67262137, ..., 0.26121438, 0.38928765,\n",
      "        0.8517241 ],\n",
      "       [0.23604253, 0.25179923, 0.47789547, ..., 0.6160306 , 0.26347324,\n",
      "        0.61831427],\n",
      "       [0.36642176, 0.5907126 , 0.3342111 , ..., 0.44528133, 0.2965043 ,\n",
      "        0.16885595],\n",
      "       [0.19733125, 2.1324568 , 0.19084528, ..., 0.00546371, 0.03202579,\n",
      "        0.06647418],\n",
      "       [0.06316034, 0.35400715, 0.03654467, ..., 0.36800697, 0.33288354,\n",
      "        0.02022352]], dtype=float32)\n",
      "array([[   3,   33, 3255,    6,    5, 3343,   10,    1,  104, 1562,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
      "       [ 256,    9, 2136,   25,  131,    6,    5,  160,   10,    1, 2129,\n",
      "           9,   18,   58, 3378,    7,    1,   28, 3516,    2],\n",
      "       [   3,    9,  102,  266,   10,    1,  369,  235,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
      "       [   3,    9,   12,   18,  154,   12,    8,  170,  247,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
      "       [ 508,  246,   14,   94,   82,    4,    5, 2945,    2,    2,    2,\n",
      "           2,    2,    2,    2,    2,    2,    2,    2,    2]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "array(['449352117.jpg', '22779038.jpg', '1916832697.jpg', '415793623.jpg',\n",
      "       '2878578240.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData(object):#读取提取的图片特征\n",
    "    def __init__(self,\n",
    "                 img_name_to_token_ids,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,#在batch中让文本对齐\n",
    "                 vocab,\n",
    "                 deterministic = False):#是否可以shuffle\n",
    "        self._vocab = vocab\n",
    "        self._all_img_feature_filepaths = []#获得文件夹下所有子文件的名字\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "\n",
    "        self._img_name_to_token_ids = img_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0#指示遍历到数据集的哪个部分了\n",
    "        self._deterministic = deterministic\n",
    "        self._img_feature_filenames = []#存储所有图片名字\n",
    "        self._img_feature_data = []#存储所有图片提取出来的向量\n",
    "        self._load_img_feature_pickle()#载入所有pickle文件\n",
    "        if not self._deterministic:#是否可以shuffle\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def _load_img_feature_pickle(self):#载入所有pickle文件\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f)\n",
    "                self._img_feature_filenames += filenames#此时是两个列表做合并\n",
    "                self._img_feature_data.append(features)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)#合并_img_feature_data\n",
    "        #[（1000,1,1,2048），（1000,1,1,2048）] -> [2000,1,1,2048]\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data, (origin_shape[0], origin_shape[3]))#[2000,1,1,2048] -> [2000,2048]\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)#变成numpy数据格式因为shuffler时用的permutation是numpy的API\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "\n",
    "    def _img_desc(self, filenames):#找到图片描述并对图片描述统一（截断，填充）\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._img_name_to_token_ids[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)#随机选一个描述\n",
    "            #chosen_token_ids = token_ids_set[0]\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:#截断\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:                                         #填充\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):#进行batch size操作\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        batch_img_names = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_img_names)#通过图片名找到图片描述\n",
    "        #weights用于统计一个描述里哪些是有用的哪些是填充的\n",
    "        #如[30,1175,10,3,0,0,0] -> [1,1,1,1,0,0,0],在算梯度时后三个不参与计算，减少计算量，提高准确率\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_img_names\n",
    "\n",
    "\n",
    "caption_data = ImageCaptionData(img_name_to_token_ids, input_img_feature_dir, hps.num_timesteps, vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ed0cdd62a79b>:43: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/kernel:0_grad is illegal; using image_feature_embed/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/bias:0_grad is illegal; using image_feature_embed/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/kernel:0_grad is illegal; using fc/logits/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/bias:0_grad is illegal; using fc/logits/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):#RNN大小，RNN种类\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):#封装dropout\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    img_feature  = tf.placeholder(tf.float32, (batch_size, img_feature_dim))#图像特征\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))#图像描述的句子\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))#weight\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)#训练的次数\n",
    "\n",
    "    #训练流程 sentence[a,b,c,d,e]\n",
    "    #         img_feature[0.4,0.3,10,2]\n",
    "    #         predict1: img_feature -> embedding_img -> lstm -> a\n",
    "    #         predict2: a -> embedding_word -> lstm -> b\n",
    "    #         input[img,a,b,c,d]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],#词表大小，每个词应该的大小\n",
    "            tf.float32)\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])#[batch_size,num_timesteps-1,num_embedding_nodes]\n",
    "        #从embedding表里查询                                                                #-1是因为最后一个字符只作为输出不作为输入\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)#对feature做全连接让其和embedding_nodes一样大\n",
    "    with tf.variable_scope('image_feature_embed', initializer=img_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)#[batch_size,num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)#[batch_size,1,num_embedding_nodes]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)#拼接\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)#对多层cell连接\n",
    "\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_node[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,#多个输入需要dynamicRNN，返回output以及中间状态\n",
    "                                           embed_inputs,\n",
    "                                           initial_state=initial_state)\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout, vocab_size, name='logits')\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])#展平句子\n",
    "        mask_flatten = tf.reshape(mask, [-1])#展平mask\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            \n",
    "            logits=logits, labels=sentence_flatten)\n",
    "        weighted_softmax_loss = tf.multiply(softmax_loss,\n",
    "                                            tf.cast(mask_flatten, tf.float32))\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type = tf.int32)#预测值，1是axis\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)#正确预测的值\n",
    "        correct_prediction_with_mask = tf.multiply(\n",
    "            tf.cast(correct_prediction, tf.float32),\n",
    "            mask_flatten)\n",
    "        accuracy = tf.reduce_sum(correct_prediction_with_mask) / mask_sum\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(#应用梯度\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)#直方图\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "            global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:[*] Reading checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from .\\data\\local_run\\image_caption-54000\n",
      "INFO:tensorflow:[*] Success Read Checkpoint From image_caption-54000\n",
      "INFO:tensorflow:Step: 54100, loss: 3.379, accuracy: 0.336\n",
      "INFO:tensorflow:Step: 54200, loss: 3.580, accuracy: 0.336\n",
      "INFO:tensorflow:Step: 54300, loss: 3.261, accuracy: 0.352\n",
      "INFO:tensorflow:Step: 54400, loss: 3.613, accuracy: 0.340\n",
      "INFO:tensorflow:Step: 54500, loss: 3.694, accuracy: 0.306\n",
      "INFO:tensorflow:Step: 54600, loss: 3.593, accuracy: 0.314\n",
      "INFO:tensorflow:Step: 54700, loss: 3.327, accuracy: 0.377\n",
      "INFO:tensorflow:Step: 54800, loss: 3.396, accuracy: 0.385\n",
      "INFO:tensorflow:Step: 54900, loss: 3.351, accuracy: 0.354\n",
      "INFO:tensorflow:Step: 55000, loss: 3.355, accuracy: 0.361\n",
      "INFO:tensorflow:Step: 55000, image caption model saved\n",
      "INFO:tensorflow:Step: 55100, loss: 3.292, accuracy: 0.381\n",
      "INFO:tensorflow:Step: 55200, loss: 3.490, accuracy: 0.370\n",
      "INFO:tensorflow:Step: 55300, loss: 2.586, accuracy: 0.504\n",
      "INFO:tensorflow:Step: 55400, loss: 2.894, accuracy: 0.464\n",
      "INFO:tensorflow:Step: 55500, loss: 2.975, accuracy: 0.423\n",
      "INFO:tensorflow:Step: 55600, loss: 3.026, accuracy: 0.434\n",
      "INFO:tensorflow:Step: 55700, loss: 2.929, accuracy: 0.463\n",
      "INFO:tensorflow:Step: 55800, loss: 3.054, accuracy: 0.403\n",
      "INFO:tensorflow:Step: 55900, loss: 2.859, accuracy: 0.442\n",
      "INFO:tensorflow:Step: 56000, loss: 2.753, accuracy: 0.509\n",
      "INFO:tensorflow:Step: 56000, image caption model saved\n",
      "INFO:tensorflow:Step: 56100, loss: 2.870, accuracy: 0.462\n",
      "INFO:tensorflow:Step: 56200, loss: 2.817, accuracy: 0.479\n",
      "INFO:tensorflow:Step: 56300, loss: 2.916, accuracy: 0.472\n",
      "INFO:tensorflow:Step: 56400, loss: 2.860, accuracy: 0.483\n",
      "INFO:tensorflow:Step: 56500, loss: 2.504, accuracy: 0.497\n",
      "INFO:tensorflow:Step: 56600, loss: 2.623, accuracy: 0.502\n",
      "INFO:tensorflow:Step: 56700, loss: 2.884, accuracy: 0.451\n",
      "INFO:tensorflow:Step: 56800, loss: 2.685, accuracy: 0.488\n",
      "INFO:tensorflow:Step: 56900, loss: 2.936, accuracy: 0.436\n",
      "INFO:tensorflow:Step: 57000, loss: 2.810, accuracy: 0.485\n",
      "INFO:tensorflow:Step: 57000, image caption model saved\n",
      "INFO:tensorflow:Step: 57100, loss: 2.559, accuracy: 0.493\n",
      "INFO:tensorflow:Step: 57200, loss: 2.512, accuracy: 0.521\n",
      "INFO:tensorflow:Step: 57300, loss: 2.611, accuracy: 0.506\n",
      "INFO:tensorflow:Step: 57400, loss: 2.526, accuracy: 0.513\n",
      "INFO:tensorflow:Step: 57500, loss: 2.634, accuracy: 0.482\n",
      "INFO:tensorflow:Step: 57600, loss: 2.441, accuracy: 0.521\n",
      "INFO:tensorflow:Step: 57700, loss: 2.882, accuracy: 0.475\n",
      "INFO:tensorflow:Step: 57800, loss: 2.516, accuracy: 0.502\n",
      "INFO:tensorflow:Step: 57900, loss: 2.523, accuracy: 0.510\n",
      "INFO:tensorflow:Step: 58000, loss: 2.660, accuracy: 0.505\n",
      "INFO:tensorflow:Step: 58000, image caption model saved\n",
      "INFO:tensorflow:Step: 58100, loss: 2.385, accuracy: 0.548\n",
      "INFO:tensorflow:Step: 58200, loss: 2.568, accuracy: 0.511\n",
      "INFO:tensorflow:Step: 58300, loss: 2.537, accuracy: 0.509\n",
      "INFO:tensorflow:Step: 58400, loss: 2.340, accuracy: 0.563\n",
      "INFO:tensorflow:Step: 58500, loss: 2.567, accuracy: 0.525\n",
      "INFO:tensorflow:Step: 58600, loss: 2.289, accuracy: 0.540\n",
      "INFO:tensorflow:Step: 58700, loss: 2.500, accuracy: 0.504\n",
      "INFO:tensorflow:Step: 58800, loss: 2.568, accuracy: 0.520\n",
      "INFO:tensorflow:Step: 58900, loss: 2.523, accuracy: 0.526\n",
      "INFO:tensorflow:Step: 59000, loss: 2.482, accuracy: 0.523\n",
      "INFO:tensorflow:Step: 59000, image caption model saved\n",
      "INFO:tensorflow:Step: 59100, loss: 2.640, accuracy: 0.503\n",
      "INFO:tensorflow:Step: 59200, loss: 2.526, accuracy: 0.516\n",
      "INFO:tensorflow:Step: 59300, loss: 2.460, accuracy: 0.522\n",
      "INFO:tensorflow:Step: 59400, loss: 2.537, accuracy: 0.532\n",
      "INFO:tensorflow:Step: 59500, loss: 2.540, accuracy: 0.506\n",
      "INFO:tensorflow:Step: 59600, loss: 2.259, accuracy: 0.556\n",
      "INFO:tensorflow:Step: 59700, loss: 2.599, accuracy: 0.523\n",
      "INFO:tensorflow:Step: 59800, loss: 2.301, accuracy: 0.555\n",
      "INFO:tensorflow:Step: 59900, loss: 2.396, accuracy: 0.537\n",
      "INFO:tensorflow:Step: 60000, loss: 2.480, accuracy: 0.527\n",
      "INFO:tensorflow:Step: 60000, image caption model saved\n",
      "INFO:tensorflow:Step: 60100, loss: 2.299, accuracy: 0.575\n",
      "INFO:tensorflow:Step: 60200, loss: 2.712, accuracy: 0.492\n",
      "INFO:tensorflow:Step: 60300, loss: 2.384, accuracy: 0.551\n",
      "INFO:tensorflow:Step: 60400, loss: 2.472, accuracy: 0.556\n",
      "INFO:tensorflow:Step: 60500, loss: 2.279, accuracy: 0.572\n",
      "INFO:tensorflow:Step: 60600, loss: 2.729, accuracy: 0.473\n",
      "INFO:tensorflow:Step: 60700, loss: 2.431, accuracy: 0.558\n",
      "INFO:tensorflow:Step: 60800, loss: 2.510, accuracy: 0.541\n",
      "INFO:tensorflow:Step: 60900, loss: 2.681, accuracy: 0.482\n",
      "INFO:tensorflow:Step: 61000, loss: 2.447, accuracy: 0.563\n",
      "INFO:tensorflow:Step: 61000, image caption model saved\n",
      "INFO:tensorflow:Step: 61100, loss: 2.453, accuracy: 0.547\n",
      "INFO:tensorflow:Step: 61200, loss: 2.297, accuracy: 0.571\n",
      "INFO:tensorflow:Step: 61300, loss: 2.461, accuracy: 0.580\n",
      "INFO:tensorflow:Step: 61400, loss: 2.204, accuracy: 0.571\n",
      "INFO:tensorflow:Step: 61500, loss: 2.342, accuracy: 0.562\n",
      "INFO:tensorflow:Step: 61600, loss: 2.144, accuracy: 0.578\n",
      "INFO:tensorflow:Step: 61700, loss: 2.276, accuracy: 0.563\n",
      "INFO:tensorflow:Step: 61800, loss: 2.381, accuracy: 0.555\n",
      "INFO:tensorflow:Step: 61900, loss: 2.221, accuracy: 0.604\n",
      "INFO:tensorflow:Step: 62000, loss: 2.613, accuracy: 0.506\n",
      "INFO:tensorflow:Step: 62000, image caption model saved\n",
      "INFO:tensorflow:Step: 62100, loss: 2.179, accuracy: 0.592\n",
      "INFO:tensorflow:Step: 62200, loss: 2.271, accuracy: 0.580\n",
      "INFO:tensorflow:Step: 62300, loss: 2.448, accuracy: 0.533\n",
      "INFO:tensorflow:Step: 62400, loss: 2.256, accuracy: 0.556\n",
      "INFO:tensorflow:Step: 62500, loss: 2.173, accuracy: 0.583\n",
      "INFO:tensorflow:Step: 62600, loss: 2.446, accuracy: 0.560\n",
      "INFO:tensorflow:Step: 62700, loss: 2.406, accuracy: 0.532\n",
      "INFO:tensorflow:Step: 62800, loss: 2.272, accuracy: 0.576\n",
      "INFO:tensorflow:Step: 62900, loss: 2.429, accuracy: 0.559\n",
      "INFO:tensorflow:Step: 63000, loss: 2.157, accuracy: 0.595\n",
      "INFO:tensorflow:Step: 63000, image caption model saved\n",
      "INFO:tensorflow:Step: 63100, loss: 2.376, accuracy: 0.534\n",
      "INFO:tensorflow:Step: 63200, loss: 2.884, accuracy: 0.477\n",
      "INFO:tensorflow:Step: 63300, loss: 2.401, accuracy: 0.560\n",
      "INFO:tensorflow:Step: 63400, loss: 2.530, accuracy: 0.513\n",
      "INFO:tensorflow:Step: 63500, loss: 2.176, accuracy: 0.593\n",
      "INFO:tensorflow:Step: 63600, loss: 2.269, accuracy: 0.562\n",
      "INFO:tensorflow:Step: 63700, loss: 2.445, accuracy: 0.532\n",
      "INFO:tensorflow:Step: 63800, loss: 2.106, accuracy: 0.618\n",
      "INFO:tensorflow:Step: 63900, loss: 2.106, accuracy: 0.577\n",
      "INFO:tensorflow:Step: 64000, loss: 2.406, accuracy: 0.538\n",
      "INFO:tensorflow:Step: 64000, image caption model saved\n",
      "INFO:tensorflow:Step: 64100, loss: 2.431, accuracy: 0.551\n",
      "INFO:tensorflow:Step: 64200, loss: 2.326, accuracy: 0.569\n",
      "INFO:tensorflow:Step: 64300, loss: 2.298, accuracy: 0.569\n",
      "INFO:tensorflow:Step: 64400, loss: 2.357, accuracy: 0.554\n",
      "INFO:tensorflow:Step: 64500, loss: 2.282, accuracy: 0.574\n",
      "INFO:tensorflow:Step: 64600, loss: 2.356, accuracy: 0.555\n",
      "INFO:tensorflow:Step: 64700, loss: 2.164, accuracy: 0.581\n",
      "INFO:tensorflow:Step: 64800, loss: 2.268, accuracy: 0.574\n",
      "INFO:tensorflow:Step: 64900, loss: 2.535, accuracy: 0.551\n",
      "INFO:tensorflow:Step: 65000, loss: 2.475, accuracy: 0.542\n",
      "INFO:tensorflow:Step: 65000, image caption model saved\n",
      "INFO:tensorflow:Step: 65100, loss: 2.113, accuracy: 0.605\n",
      "INFO:tensorflow:Step: 65200, loss: 2.226, accuracy: 0.580\n",
      "INFO:tensorflow:Step: 65300, loss: 2.423, accuracy: 0.545\n",
      "INFO:tensorflow:Step: 65400, loss: 2.322, accuracy: 0.566\n",
      "INFO:tensorflow:Step: 65500, loss: 2.179, accuracy: 0.601\n",
      "INFO:tensorflow:Step: 65600, loss: 2.269, accuracy: 0.552\n",
      "INFO:tensorflow:Step: 65700, loss: 2.311, accuracy: 0.562\n",
      "INFO:tensorflow:Step: 65800, loss: 2.134, accuracy: 0.591\n",
      "INFO:tensorflow:Step: 65900, loss: 2.112, accuracy: 0.599\n",
      "INFO:tensorflow:Step: 66000, loss: 2.233, accuracy: 0.571\n",
      "INFO:tensorflow:Step: 66000, image caption model saved\n",
      "INFO:tensorflow:Step: 66100, loss: 2.162, accuracy: 0.586\n",
      "INFO:tensorflow:Step: 66200, loss: 2.221, accuracy: 0.568\n",
      "INFO:tensorflow:Step: 66300, loss: 2.336, accuracy: 0.551\n",
      "INFO:tensorflow:Step: 66400, loss: 2.277, accuracy: 0.566\n",
      "INFO:tensorflow:Step: 66500, loss: 2.414, accuracy: 0.535\n",
      "INFO:tensorflow:Step: 66600, loss: 2.404, accuracy: 0.550\n",
      "INFO:tensorflow:Step: 66700, loss: 2.177, accuracy: 0.584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step: 66800, loss: 2.357, accuracy: 0.544\n",
      "INFO:tensorflow:Step: 66900, loss: 2.652, accuracy: 0.520\n",
      "INFO:tensorflow:Step: 67000, loss: 2.184, accuracy: 0.580\n",
      "INFO:tensorflow:Step: 67000, image caption model saved\n",
      "INFO:tensorflow:Step: 67100, loss: 2.310, accuracy: 0.545\n",
      "INFO:tensorflow:Step: 67200, loss: 2.192, accuracy: 0.580\n",
      "INFO:tensorflow:Step: 67300, loss: 2.247, accuracy: 0.553\n",
      "INFO:tensorflow:Step: 67400, loss: 2.105, accuracy: 0.587\n",
      "INFO:tensorflow:Step: 67500, loss: 2.491, accuracy: 0.541\n",
      "INFO:tensorflow:Step: 67600, loss: 2.258, accuracy: 0.576\n",
      "INFO:tensorflow:Step: 67700, loss: 2.267, accuracy: 0.568\n",
      "INFO:tensorflow:Step: 67800, loss: 2.180, accuracy: 0.600\n",
      "INFO:tensorflow:Step: 67900, loss: 2.392, accuracy: 0.537\n",
      "INFO:tensorflow:Step: 68000, loss: 2.315, accuracy: 0.562\n",
      "INFO:tensorflow:Step: 68000, image caption model saved\n",
      "INFO:tensorflow:Step: 68100, loss: 2.434, accuracy: 0.545\n",
      "INFO:tensorflow:Step: 68200, loss: 2.058, accuracy: 0.607\n",
      "INFO:tensorflow:Step: 68300, loss: 2.448, accuracy: 0.555\n",
      "INFO:tensorflow:Step: 68400, loss: 2.182, accuracy: 0.597\n",
      "INFO:tensorflow:Step: 68500, loss: 2.140, accuracy: 0.604\n",
      "INFO:tensorflow:Step: 68600, loss: 2.312, accuracy: 0.578\n",
      "INFO:tensorflow:Step: 68700, loss: 2.327, accuracy: 0.579\n",
      "INFO:tensorflow:Step: 68800, loss: 2.215, accuracy: 0.566\n",
      "INFO:tensorflow:Step: 68900, loss: 2.205, accuracy: 0.576\n",
      "INFO:tensorflow:Step: 69000, loss: 2.294, accuracy: 0.569\n",
      "INFO:tensorflow:Step: 69000, image caption model saved\n",
      "INFO:tensorflow:Step: 69100, loss: 2.370, accuracy: 0.558\n",
      "INFO:tensorflow:Step: 69200, loss: 2.299, accuracy: 0.556\n",
      "INFO:tensorflow:Step: 69300, loss: 2.449, accuracy: 0.550\n",
      "INFO:tensorflow:Step: 69400, loss: 2.216, accuracy: 0.572\n",
      "INFO:tensorflow:Step: 69500, loss: 2.268, accuracy: 0.586\n",
      "INFO:tensorflow:Step: 69600, loss: 2.051, accuracy: 0.612\n",
      "INFO:tensorflow:Step: 69700, loss: 2.346, accuracy: 0.537\n",
      "INFO:tensorflow:Step: 69800, loss: 2.002, accuracy: 0.621\n",
      "INFO:tensorflow:Step: 69900, loss: 2.154, accuracy: 0.577\n",
      "INFO:tensorflow:Step: 70000, loss: 2.198, accuracy: 0.574\n",
      "INFO:tensorflow:Step: 70000, image caption model saved\n",
      "INFO:tensorflow:Step: 70100, loss: 2.053, accuracy: 0.605\n",
      "INFO:tensorflow:Step: 70200, loss: 2.417, accuracy: 0.542\n",
      "INFO:tensorflow:Step: 70300, loss: 2.313, accuracy: 0.568\n",
      "INFO:tensorflow:Step: 70400, loss: 2.339, accuracy: 0.550\n",
      "INFO:tensorflow:Step: 70500, loss: 2.053, accuracy: 0.608\n",
      "INFO:tensorflow:Step: 70600, loss: 2.438, accuracy: 0.553\n",
      "INFO:tensorflow:Step: 70700, loss: 2.118, accuracy: 0.586\n",
      "INFO:tensorflow:Step: 70800, loss: 2.215, accuracy: 0.579\n",
      "INFO:tensorflow:Step: 70900, loss: 2.283, accuracy: 0.568\n"
     ]
    }
   ],
   "source": [
    "training_steps = 50000\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init_op)\n",
    "    logging.info(\"[*] Reading checkpoint ...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(output_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(output_dir, ckpt_name))\n",
    "        logging.info(\"[*] Success Read Checkpoint From %s\" % (ckpt_name))\n",
    "    #else:\n",
    "        #raise Exception(\"[*] Failed load checkpoint\")\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(training_steps):\n",
    "        batch_img_features, batch_sentence_ids, batch_weights, _ = caption_data.next(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "\n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "        outputs = sess.run(fetches, feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[4]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accuracy: %3.3f'\n",
    "                         % (global_step_val, loss_val, accuracy_val))\n",
    "        if should_save:\n",
    "            logging.info(\"Step: %d, image caption model saved\" % (global_step_val))\n",
    "            saver.save(sess, os.path.join(output_dir, \"image_caption\"), global_step=global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
