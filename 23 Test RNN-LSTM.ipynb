{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#流程\n",
    "#构建计算图-LSTM模型:\n",
    "#    embedding\n",
    "#    LSTM\n",
    "#    fc\n",
    "#    train_op\n",
    "#训练流程代码\n",
    "#数据集代码\n",
    "#    api：next_batch(batch_size)\n",
    "#词表封装\n",
    "#    api：sentence2id(sentence)句子转化id\n",
    "# 类别的封装\n",
    "#    api: category2id(text_category).\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)#在notebook中打印日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():#定义LSTM模型需要的参数\n",
    "    return tf.contrib.training.HParams(#这个api用于管理参数，返回一个对象\n",
    "        # for fully run\n",
    "        # num_embedding_size = 32,\n",
    "        # num_timesteps = 600,\n",
    "        # num_lstm_nodes = [128, 128],\n",
    "        # num_lstm_layers = 2,\n",
    "        # num_fc_nodes = 128,\n",
    "        num_embedding_size = 16,\n",
    "        num_timesteps = 50,#对齐mini_batch,指定步长,数据在mini_batch里被截断\n",
    "        num_lstm_nodes = [32, 32],#LSTM每一层的size，神经单元数\n",
    "        num_lstm_layers = 2,#LSTM层次，2层，每层32个神经单元\n",
    "        num_fc_nodes = 32,#fc的神经单元数\n",
    "        batch_size = 100,\n",
    "        clip_lstm_grads = 1.0,#控制梯度大小\n",
    "        learning_rate = 0.001,\n",
    "        num_word_threshold = 10,#忽略掉出现次数不够的词语\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = '.\\\\cnews_data\\\\cnews.train.seg.txt'\n",
    "val_file = '.\\\\cnews_data\\\\cnews.val.seg.txt'\n",
    "test_file = '.\\\\cnews_data\\\\cnews.test.seg.txt'\n",
    "vocab_file = '.\\\\cnews_data\\\\cnews.vocab.txt'\n",
    "category_file = '.\\\\cnews_data\\\\cnews.category.txt'\n",
    "output_folder = '.\\\\run_text_rnn'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 77323\n",
      "INFO:tensorflow:num_classes: 10\n",
      "INFO:tensorflow:label: 时尚, id: 5\n"
     ]
    }
   ],
   "source": [
    "class Vocab:#词表封装模块\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)#词语不存在就返回unk的id\n",
    "    \n",
    "    @property#成员函数\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):#把句子转化成一系列id\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\n",
    "                \"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info('vocab_size: %d' % vocab_size)\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('num_classes: %d' % num_classes)\n",
    "\n",
    "test_str = '时尚'\n",
    "tf.logging.info(\n",
    "    'label: %s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from .\\cnews_data\\cnews.train.seg.txt\n",
      "INFO:tensorflow:Loading data from .\\cnews_data\\cnews.val.seg.txt\n",
      "INFO:tensorflow:Loading data from .\\cnews_data\\cnews.test.seg.txt\n",
      "50000\n",
      "5000\n",
      "10000\n",
      "(array([[   21,     0,  3570,     8,  2243,   289,  3291,     9, 10532,\n",
      "         1433,   128,  2250,   788,    27,    72,    23,   753,    44,\n",
      "        12203, 33116, 46780,     0,    66,     1,    93,    21,  4346,\n",
      "        18228, 49282,  3384,     1,    22, 23658, 39154,  3291,     1,\n",
      "           22,   484,  3329,  5444,   289,  3291,     5,  2243,     0,\n",
      "           10,  1369,   289,  3291,     1],\n",
      "       [   15,  3796,  3796,    79,  5980,    14, 35936,     0,  4017,\n",
      "         6117,  1644,    40,  3796,  3796,    79, 14285,  6063,   644,\n",
      "         1590,  1400,   141,     2, 10313,     1,  7353,     0,     1,\n",
      "         3796,  3796,  4753,   130,  7753,    75,     1,   293,   594,\n",
      "         7039,    92,   771,  5234,    92,     0, 15640,     5, 13171,\n",
      "          789,     5,   224,  3438,     5]]), array([4, 7]))\n",
      "(array([[    0,     0,     0, 29421, 11925,   971,   593,  3518,  8777,\n",
      "        48118, 66921,   156,   708,  3518,     2,     0,     1, 39904,\n",
      "         8130,     2,   378,     6,    31,  1305,     1,   146, 42726,\n",
      "           30, 13618,  9839, 14133,    30,  1584,  2408,     3,  1584,\n",
      "          670, 35203,   220,     7,  4212,     1, 13618,   304,  7293,\n",
      "           41,     1,     0,   901,    52],\n",
      "       [   43,    86,  2109, 26431,  1909,   879,  3833,  5072,  1070,\n",
      "        33874,  5376,    59,  1346,  1478,    21,     1,  2812,   709,\n",
      "          879,  2237,  1338,  1111,     1,   176,   215,    43,   404,\n",
      "         6729,   168,   682,   471,     1,   229,  1030,     2,    43,\n",
      "         1866,  1338,  1733,    63,   255,  1663,     1,   288,    43,\n",
      "            2,  1136,   168,  1347,  1588]]), array([0, 9]))\n",
      "(array([[ 2341,  2882, 38869, 47461, 60277, 15461, 45388,  2144,   253,\n",
      "          527,  1933,    11,     0, 13700, 25377,     2,   574,  1201,\n",
      "            4,   695,     1,   777,     2,  7419,   591,   695, 29331,\n",
      "            3,  2815, 14846,   644,   579,     1,  9240,   455,  1317,\n",
      "           24,   238,     6,   607,   296,     2, 23855,   669,    74,\n",
      "           74, 14846, 10558,     5, 14846],\n",
      "       [  467,    11,    15,  4097,    14,  5591,  1146,  2352,  4125,\n",
      "        29511, 12295,   156,   481,   456,    99,    15,  4097,    14,\n",
      "           17, 26637, 60016,    16,   660,    78,   130,    23,   174,\n",
      "           44,     4,   640,   226,  5591,  1146,    17, 67632, 35469,\n",
      "        67376,    16,  2352,     1,  1205,   223,  4125,    17,  8961,\n",
      "            0,    16,  1585,     1,    19]]), array([8, 1]))\n"
     ]
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):#输入文件名，词典，类别词典，对齐限制数\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []#输入的文本,为矩阵\n",
    "        # vector\n",
    "        self._outputs = []#输出的类别值，为向量\n",
    "        self._indicator = 0#指明当前batch已经到了数据的哪个位置\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from %s', filename)\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)#把label值换成id\n",
    "            id_words = self._vocab.sentence_to_id(content)#把句子换成id\n",
    "            id_words = id_words[0: self._num_timesteps]#对行进行截断，如果过长\n",
    "            padding_num = self._num_timesteps - len(id_words)#计算需要填充的量\n",
    "            id_words = id_words + [#对行进行填充，如果不够长，填充unk\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)#加入输入\n",
    "            self._outputs.append(id_label)#加入输出\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)#转成numpy的矩阵\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)#转成numpy的矩阵\n",
    "        self._random_shuffle()#随机化函数\n",
    "        self._num_examples = len(self._inputs)\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "            \n",
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, hps.num_timesteps) \n",
    "val_dataset = TextDataSet(\n",
    "    val_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.num_examples())\n",
    "print(val_dataset.num_examples())\n",
    "print(test_dataset.num_examples())\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(val_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-4f663c7f8812>:91: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: embedding/embedding:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/outputs/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/outputs/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/outputs/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/h_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/biases:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n"
     ]
    }
   ],
   "source": [
    "#构建计算图-LSTM模型:\n",
    "#    embedding\n",
    "#    LSTM\n",
    "#    fc\n",
    "#    train_op\n",
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')#用于dropout\n",
    "    \n",
    "    global_step = tf.Variable(#保存步数\n",
    "        tf.zeros([], tf.int64), name = 'global_step', trainable=False)\n",
    "    \n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(#如果变量存在就重用，不存在就创建一个\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32)\n",
    "        # [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)#把inputs转化成embedding\n",
    "    \n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _generate_params_for_lstm_cell(x_size, h_size, bias_size):\n",
    "        \"\"\"generates parameters for pure lstm implementation.\"\"\"\n",
    "        x_w = tf.get_variable('x_weights', x_size)\n",
    "        h_w = tf.get_variable('h_weights', h_size)\n",
    "        b = tf.get_variable('biases', bias_size,initializer=tf.constant_initializer(0.0))\n",
    "        return x_w, h_w, b\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('lstm_nn', initializer = lstm_init):\n",
    "        \n",
    "        with tf.variable_scope('inputs'):#所有门参数相同\n",
    "            ix, ih, ib = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('outputs'):\n",
    "            ox, oh, ob = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('forget'):\n",
    "            fx, fh, fb = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        with tf.variable_scope('memory'):#对输入的变换\n",
    "            cx, ch, cb = _generate_params_for_lstm_cell(\n",
    "                x_size = [hps.num_embedding_size, hps.num_lstm_nodes[0]],\n",
    "                h_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                bias_size = [1, hps.num_lstm_nodes[0]]\n",
    "            )\n",
    "        state = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]),trainable = False)\n",
    "        h = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]),trainable = False)#第一层输出为0\n",
    "        \n",
    "        #LSTM结构\n",
    "        for i in range(num_timesteps):#每输入一个值都要执行一次\n",
    "            embed_input = embed_inputs[:, i, :] # [batch_size, 1, embed_size]\n",
    "            embed_input = tf.reshape(embed_input,[batch_size, hps.num_embedding_size])\n",
    "            forget_gate = tf.sigmoid(tf.matmul(embed_input, fx) + tf.matmul(h, fh) + fb)\n",
    "            input_gate = tf.sigmoid(tf.matmul(embed_input, ix) + tf.matmul(h, ih) + ib)\n",
    "            output_gate = tf.sigmoid(tf.matmul(embed_input, ox) + tf.matmul(h, oh) + ob)\n",
    "            mid_state = tf.tanh(tf.matmul(embed_input, cx) + tf.matmul(h, ch) + cb)\n",
    "            state = mid_state * input_gate + state * forget_gate\n",
    "            h = output_gate * tf.tanh(state)\n",
    "        last = h\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        fc1 = tf.layers.dense(last, \n",
    "                              hps.num_fc_nodes,\n",
    "                              activation = tf.nn.relu,\n",
    "                              name = 'fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name = 'fc2')\n",
    "    \n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits, labels = outputs)#1.把labels做onehot，2.对logits计算概率，3.算loss\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0, 1, 5, 4, 2] -> argmax: 2\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits),1, output_type = tf.int32)#logits=10*1，在列方向看谁最大\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    with tf.name_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()#获得所有可以train的变量\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: %s' % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(#对梯度做截断，参数（所有的梯度在所有可训练的变量上，限制值））\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(#应用梯度，参数（梯度和变量的列表，）\n",
    "            zip(grads, tvars), global_step = global_step)\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_holdout(sess, accuracy, dataset_for_test, batch_size):\n",
    "    num_batches = dataset_for_test.num_examples() // batch_size\n",
    "    tf.logging.info(\"Eval holdout: num_examples = %d, batch_size = %d\",\n",
    "                    dataset_for_test.num_examples(), batch_size)\n",
    "    accuracy_vals = []\n",
    "    for i in range(num_batches):\n",
    "        batch_inputs, batch_labels = dataset_for_test.next_batch(batch_size)\n",
    "        accuracy_val = sess.run(accuracy,\n",
    "                                feed_dict = {\n",
    "                                    inputs: batch_inputs,\n",
    "                                    outputs: batch_labels,\n",
    "                                    keep_prob: 1.0,\n",
    "                                })\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "    return np.mean(accuracy_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   200, loss: 2.171, accuracy: 0.170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dadee0b0b0eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                                    \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                    \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                                    \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_keep_prob_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                                })\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "num_train_steps = 10000\n",
    "\n",
    "# Train: 99.7%\n",
    "# Valid: 92.7%\n",
    "# Test:  93.2%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict = {\n",
    "                                   inputs: batch_inputs,\n",
    "                                   outputs: batch_labels,\n",
    "                                   keep_prob: train_keep_prob_value,\n",
    "                               })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 200 == 0:\n",
    "            tf.logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\"\n",
    "                            % (global_step_val, loss_val, accuracy_val))\n",
    "        \n",
    "        if global_step_val % 1000 == 0:\n",
    "            accuracy_eval = eval_holdout(sess, accuracy, val_dataset, hps.batch_size)\n",
    "            accuracy_test = eval_holdout(sess, accuracy, test_dataset, hps.batch_size)\n",
    "            tf.logging.info(\"Step: %5d, val_accuracy: %3.3f, test_accuracy: %3.3f\"\n",
    "                            % (global_step_val, accuracy_eval, accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
